# The good, the bad and the ugly (and everything in between)

## TL;DR
First things first, everything here is solely my personal opinion.  
Teh GRPC support in Google Cloud endpoints is really great and even more important really easy to use. It already brings a lot of functionality out-of-the-box like request quotas, API key support and so on. A basic setup can be done within a very reasonable timespan.  
But of course it won't get you all the way (just yet).
You will still need to implement your own customer-based monitoring (and billing of course) and hosting a developer frontend for your API is also up to you.

## More details please...

Ok, let's dig a little deeper into each point we visited during this tutorial.

### Deploying your API
This is really a breeze. You simply create your GRPC API Description and a slim config file and are good to go.
You will get a basic monitoring of-the-shelf and as candy topping your configuration uploads are automatically versioned so you can track your changes.
The ESP is really an great project and works very well with your basic setup. Documentation is very good and eases your first steps. The developer portal which you can activate is a nice gimmick but as it's only available to someone after you activly granted access simply wastes a lot of potential. Hopefully Google changes this in the future and decouples the access policies of your API and it's developer portal. (there are [thoughts](https://groups.google.com/d/msg/google-cloud-endpoints/k6wY0WHHJHg/f8swCYNfAwAJ) at Google, but no comittment)

### Using API keys
Pretty much the same as above. Getting started is as simple as changing one value in your API config. So thumbs up for that.  
However API keys need to be generated by yourself and even worse there is currently no CLI support for that. You need to actually open up the Web Console and manually click through the creation process. Even though that realy takes a couple of seconds, it's simply not feasable if you want to have your API out in the world.  
The ESP already supports different auth methods like oAuth2, so it might be worth takeing a closer look there to see if something production-ready is already there and i simply didn't take it into consideration for this tutorial.

### Quotas
A great feature but you can feel it's still in BETA.
While the documentation for the tutorial was good, it get's tedious pretty fast if you want to go further.  
For example, i tried to figure out if the ESP would already support quotas which grant you a request pool per month instead of per minute.  
The bottomline: i couldn't find any information about that in the docs and was left to dig into the ESP code. (which i stopped after 5 minutes as i wanted to focus on the tutorial).
If you're thinking about payment plans for your API with different quotas for each tier you basically have 2 options (both need to be called ugly).  
First you can override your quota for a project and if i say project it means Google Cloud project. So basically your customer needs to create it's own Google Cloud project and enable your API from there, then you can set specific quoats for that project (or so it states in the documentation). To be honest, i didn't test this because i didn't want to go that way for various reasons the most important being taht i don't want to dictate to my customers that they need a Google Cloud account.  
The second would be to deploy a seperate service for each tier (i.e. yages-free-tier.my-project.cloud.goog, yages-gold-tier.my-project.cloud.goog, etc.). While this most likely will work it comes at the expanse of  
a) having multiple services for a single API to maintain  
b) having an ESP for each service running which increases resource cost
As a side note and final word on this block i should mention that during my research on quotas i came across different API management solutions with Kong Enterprise & APIGEE looking like the most promising (at least on the paper / Youtube / Powerpoint)

### TLS
TBD

### The Nasty (or better the Quirky)
For whatever reason Google decided that certain resources won't be deleted imediatly after you decided to do so, but instead are only marked for deletion and are kept around for 30 days.  
While i can think of at least one argument (recovery after accidental deletion) this is nothing less than annoying during development.  
When i started the tutorial i made some mistakes which left me the option to try repairing stuff or simply throw everything away and have a clean start.  
For the sake of the tutorial i wanted to streamline everything as much as possible and threw away my project and created a new one.  
Just to find that the ID of the project is now blocked for 30 days. Oh, the same goes for your Endpoints servies. So if you want a fresh start because you made a mess, you need to come up with a new name for your service.  
As is said, there might be good reasons to do it that way but i can't see them...

### Postmortem
Google get's you going with GRPC and API Endpoints quite quickly but if you look at the complete business case of monetizing your API it just leaves you wanting more.
In a nutshell if you're just starting out with Cloud Services, plan to use GRPC then Google Cloud might be worth a look.  
However if you already have a Cloud Provider i wouldn't advise to migrate just now as the amount of manual work will be pretty much the same on each Cloud Provider at least for the use case of monetizing your GRPC API.